# -*- coding: utf-8 -*-
"""ML model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HcOHEvcn-5GmQ-G3IPtd5KzMkCBe3b9h
"""

# Commented out IPython magic to ensure Python compatibility.

!pip install -q sklearn #Scikit-learn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines
 
# %tensorflow_version 2.x
 
from __future__ import absolute_import, division, print_function, unicode_literals #__future__ is a pseudo-module which programmers can use to enable new language features which are not compatible with the current interpreter. 
 
import numpy as np #NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.
 
import pandas as pd # Pandas is one of the tools in Machine Learning which is used for data cleaning and analysis. 
import matplotlib.pyplot as plt
 
from IPython.display import clear_output #clear the output of a cell.
from six.moves import urllib #urllib is capable of fetching URLs using a variety of different protocols.
 
import tensorflow.compat.v2.feature_column as fc
import tensorflow as tf

# Load dataset.
dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data (take csv file and read it, ctrl click to look at the data)
#1 means survived an 0 die, this can be linear regression as there is a very higher chance that females survive, thats a strong corelation, age also has a coorelation as younger you are you have a higher chance
#Similarly class can be related to survival as well. We can always assume some pattern, some linear coorelation
#training data is to train the model , test is to test the model
dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data, load intp panda data frame, allowing data to be stored in a nice form, we can refernce specific columns and stuff
print(dftrain.head()) #prints the head basically the first 5 entries
y_train = dftrain.pop('survived') #remove the survived column and store in another variable
y_eval = dfeval.pop('survived')
print(dftrain.head())#withput survived column
print(y_train.head())#only survived column



print(dftrain.loc[0])#this prints row 0, first entry
print(dftrain["age"])#prints the age column

dftrain.describe()#just gives some overall information

dftrain.shape #627 row, 9 columns

dftrain.age.hist()#plotting a histogram for age

dftrain.sex.value_counts().plot(kind="barh")

dftrain["class"].value_counts().plot(kind="barh")

pd.concat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survive') #survival rate based on gender

# Categorical data: non numeric data , we need to convert this into numbers somehow. So we encode this using integers. Female-->0, male---->1
#therefore rather than handling strings we use integers
#class column----> 1st-1, 2nd-2, 3rd-3 (encoding)

CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',
                       'embark_town', 'alone'] #non numeric columns
NUMERIC_COLUMNS = ['age', 'fare']

feature_columns = [] #what we feed to our ml model to make predictions
for feature_name in CATEGORICAL_COLUMNS: #name in each column 
  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column
  #example: print(dftrain["class"].unique())-->this would print 3 unique values in the column of class for encoding efficiently
  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)) #create a column with feature name and vocabulary associated with it
#linear regression needs to know about the columns and whether they are categorical or not
for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))

print(feature_columns)

def make_input_fn(data_df, label_df, num_epochs=199, shuffle=True, batch_size=32): #parameters: (panda dataframe, label dataframe(y_train etc), no of epochs, are we shuffling data, how many elements during training time? )
  def input_function():  # inner function, this will be returned
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label, -->pass dictionary representation of data frame and we pass label data frame and we create objects
    if shuffle:
      ds = ds.shuffle(1000)  # randomize order of data
    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs
    return ds  # return a batch of the dataset, repeat statement above executes 10 times, epoch=10, so the model sees 32 batches of data 10 times, so say the data has 64 entries, each batch of 32 would be feed into model 10 times so 20 times would be the total no of trials on data
  return input_function  # return a function object for use

train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model
eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)

linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)
# We create a linear estimtor by passing the feature columns we created earlier

linear_est.train(train_input_fn)  # train, give that input funstion (essentialy its a func object)
result = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data

clear_output()  # clears consoke output
print(result['accuracy'])  # the result variable is simply a dict of stats about our model
print(result)

from __future__ import absolute_import, division, print_function, unicode_literals


import tensorflow as tf

import pandas as pd

CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species'] #what are the columns
SPECIES = ['Setosa', 'Versicolor', 'Virginica'] #just the classes
# Lets define some constants to help us later on

train_path = tf.keras.utils.get_file(
    "iris_training.csv", "https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv") #save as iris_training and grab it from the link, Keras acts as an interface for the TensorFlow library. 
test_path = tf.keras.utils.get_file(
    "iris_test.csv", "https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv")

train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0) #header=0 implies row 0 is first entry
test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)
# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe

train.head()
train_y = train.pop('Species')
test_y = test.pop('Species')
train.head() # the species column is now gone



"""Here its cool cause all columns have numeric value so encoding required"""

train_y.head()

train.shape

def input_fn(features, labels, training=True, batch_size=256): #no epochs here
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels)) #same here, 
#from_tensor_slices() method, we can get the slices of an array in the form of objects, 

    # Shuffle and repeat if you are in training mode.
    if training:
        dataset = dataset.shuffle(1000).repeat() #shuffle information and repeat
    
    return dataset.batch(batch_size)

# Feature columns describe how to use the input.
my_feature_columns = [] #same thing, getting the final columns ready 
for key in train.keys(): #train.key() gives all columns or we can use CSV_COLUMN NAMES
    my_feature_columns.append(tf.feature_column.numeric_column(key=key))
print(my_feature_columns)

"""And now we are ready to choose a model. For classification tasks there are variety of different estimators/models that we can pick from. Some options are listed below.

DNNClassifier (Deep Neural Network)
*Better option, more information drawn in the blue thin nb

LinearClassifier #**works similar to linear regression, gets the probability of identifying with a specific label**
"""

# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.
classifier = tf.estimator.DNNClassifier(
    feature_columns=my_feature_columns,
    # Two hidden layers of 30 and 10 nodes respectively.
    hidden_units=[30, 10],
    # The model must choose between 3 classes.
    n_classes=3)

classifier.train(
    input_fn=lambda: input_fn(train, train_y, training=True),
    steps=5000)
# We include a lambda to avoid creating an inner function previously, lambda is an anynomous 1 line function, it means that this is a function with a single line, the return value of lamda is stored in input_fn
#the steps is like epochs, we go through the dataset 5000 times

eval_result = classifier.evaluate(
    input_fn=lambda: input_fn(test, test_y, training=False)) #we pass test ans test_y instead of train 

print('\nTest set accuracy: {accuracy:0.3f}\n'.format(**eval_result))

#this is for predicting on one value
def input_fn(features, batch_size=256):
    # Convert the inputs to a Dataset without labels.
    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size) #no labels cause we dont know the labels yet

features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']
predict = {} #dictionary

print("Please type numeric values as prompted.")
for feature in features:
  valid = True
  while valid: 
    val = input(feature + ": ") #for each feature we get a valid response and then add to dictionary, has to be a number 
    if not val.isdigit(): valid = False

  predict[feature] = [float(val)] #list with those values for that feature, 1 entry only for each feature  

predictions = classifier.predict(input_fn=lambda: input_fn(predict)) #the input function above
for pred_dict in predictions:
    print(pred_dict)
    
    class_id = pred_dict['class_ids'][0]
    probability = pred_dict['probabilities'][class_id]

    print('Prediction is "{}" ({:.1f}%)'.format(
        SPECIES[class_id], 100 * probability))

